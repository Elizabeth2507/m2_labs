{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Automatic Speech Recognition\n",
    "\n",
    "**Name**: \n",
    "\n",
    "This lab is an introduction to automatic speech recognition (ASR). In a nutshell, the ASR process consists of the following blocks:\n",
    "\n",
    "1. **Feature extraction**: The audio waveform is preprocessed into normalized features such as spectrogram or MFCCs.\n",
    "2. **Acoustic model**: The features are transformed into high-dimensional embeddings via a deep neural network (here: [wav2vec2](https://arxiv.org/pdf/2006.11477)), and then a classifier is applied to predict the class of each token (=character) at each time step.\n",
    "3. **Decoding**: A transcript is generated from the sequence of class probabilities, either using a language model or not.\n",
    "\n",
    "<center><a href=\"https://developer.nvidia.com/blog/how-to-build-domain-specific-automatic-speech-recognition-models-on-gpus/\">\n",
    "    <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2019/12/ASR-Pipeline-1.png\" width=\"600\"></a></center>\n",
    "    \n",
    "In this lab, we experiment with ASR using pretrained acoustic models (which include feature extraction for simplicity). We provide a dataset of real-life speech audio (with trancription) in order to study the influence of several factors onto the performance of a modern ASR system. We focus on the usage of the acoustic model, thus we use a simple decoder with no language model (this will be the topic of the next lab).\n",
    "\n",
    "This series of lab relies on [torchaudio](https://pytorch.org/audio/stable/index.html), which is a pytorch-based librairy for audio/signal processing. More specifically, this lab is based on this [tutorial](https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html#sphx-glr-tutorials-speech-recognition-pipeline-tutorial-py), which you are encouraged to check.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import IPython\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "torch.random.manual_seed(0);\n",
    "\n",
    "MAX_FILES = 100 # lower this number for processing a subset of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and display an audio example\n",
    "\n",
    "Let us first load an audio file, listen to it using an audio reader, and plot the waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path (audio and transcripts)\n",
    "data_speech_dir = \"asr-dataset/speech/\"\n",
    "data_transc_dir = \"asr-dataset/transcription/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example file\n",
    "audio_file = 'asr-dataset/speech/61-70968-0001.wav'\n",
    "waveform, sr = torchaudio.load(audio_file, channels_first=True)\n",
    "IPython.display.Audio(data=waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the speech signal waveform\n",
    "plt.figure()\n",
    "xt = torch.arange(waveform.size(1)) / sr\n",
    "plt.plot(xt, waveform.T)\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acoustic model\n",
    "\n",
    "Let us create an acoustic model that performs feature extraction and classification. In this lab, we use [wav2vec2](https://arxiv.org/pdf/2006.11477), a self-supervised audio model that produces state-of-the-art results for many audio tasks (including ASR).\n",
    "\n",
    "For convenience, both the model definition and pretrained wav2vec2 weights can be directly loaded using torchaudio, which includes two types of weights:\n",
    "- The pretrained weights [without fine-tuning](https://pytorch.org/audio/stable/pipelines.html#id3), that can be fine-tuned for any other downstream tasks (e.g., emotion recognition or language detection).\n",
    "- The pretrained weights that are [fine-tuned for the ASR task](https://pytorch.org/audio/stable/pipelines.html#id36), using an extra classification layer after feature extraction with wav2vec2.\n",
    "\n",
    "In this series of lab, we use the latter. Below we provide the code for constructing a model and fetching the pretrained weights along with the classification labels. Here we use the `WAV2VEC2_ASR_BASE_100H` model. As the name indicates, it is a basic wav2vec2 architecture that is fine-tuned for ASR using 100 hours of the Librispeech dataset (containing both speech and transcriptions). Other models can be fetched using torchaudio (see the list [here](https://pytorch.org/audio/stable/pipelines.html#id36))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the acoustic model\n",
    "model_name = 'WAV2VEC2_ASR_BASE_100H'\n",
    "bundle = getattr(torchaudio.pipelines, model_name)\n",
    "acoustic_model = bundle.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the details of the model's architecture (more info on this in the Neural Network labs!)\n",
    "print(acoustic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the classification labels\n",
    "labels = bundle.get_labels()\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the acoustic model to the example audio signal wa have loaded above. We can either compute the acoustic features or predicting the labels after the classification layer (also called the *emission* matrix/tensor):\n",
    "- `features` is a list of tensors, where each tensor is the output of a transformer layer from the wav2vec2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    features, _ = acoustic_model.extract_features(waveform)\n",
    "\n",
    "print(f\"Number of transformer layers: {len(features)}\")\n",
    "print(features[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `emission` is a tensor of size `[1, time steps, number of classes]` corresponding to the output of the classification layer (`1` corresponds to the batch size). Note that it is in the form of logits, not probabilities (thus it is not normalized in $[0, 1]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    emission, _ = acoustic_model(waveform)\n",
    "\n",
    "print(emission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Vizualize the result of the classification\n",
    "plt.figure()\n",
    "plt.imshow(emission[0].cpu().T)\n",
    "plt.title(\"Classification result\")\n",
    "plt.xlabel(\"Frame (time-axis)\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the most likely label at each time frame to get a first rough transcript estimate, as done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.argmax(emission[0], dim=-1)  # take the most likely index at each time step\n",
    "pred_labels = [labels[i] for i in indices] # transform each index into the corresponding label\n",
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are many blank labels `'-'`, as well as duplicates. This is why some further postprocessing / decoding is needed.\n",
    "\n",
    "## Decoding\n",
    "\n",
    "From the emission tensor, which is the sequence of probabilities (or logits) for each class / token, we now want to generate a transcript (or *hypothesis*). This process is called *decoding*. Decoding can be complex because it implies accounting for the context in order to avoid duplicates, assembling characters into words, identifying precise word start / end, and building sequences of words that make sense given some underlying language model.\n",
    "\n",
    "Such refined decoding techniques involve using external ressources such as lexicon and language models. In this lab however, we rely on a naive approach called a *greedy* decoder, which does not depend on such components. The context information is not used (= we pick the best hypothesis at each time step), and only one transcript is generated by applying some basic post-processing (removing blank tokens and duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class GreedyDecoder(torch.nn.Module):\n",
    "    def __init__(self, labels, blank_token_indx=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank_token_indx = blank_token_indx\n",
    "\n",
    "    def forward(self, emission):\n",
    "        \"\"\"Given a sequence emission over labels, decode the transcript\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[1, num_seq, num_label]`.\n",
    "        Returns:\n",
    "          transcript (List of strings): The resulting transcript\n",
    "        \"\"\"\n",
    "        emission = emission[0] # take the first element in the batch (only one)\n",
    "        indices = torch.argmax(emission, dim=-1)  # take the most likely index at each time step\n",
    "        indices = torch.unique_consecutive(indices, dim=-1) # remove duplicates\n",
    "        indices = [i for i in indices if i != self.blank_token_indx] # remove blank token\n",
    "        transcript = \"\".join([self.labels[i] for i in indices]) # convert indices into tokens\n",
    "        transcript = transcript.replace(\"|\", \" \").lower().split() # a bit of post-processing\n",
    "        return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate the decoder, apply it, and display the result\n",
    "decoder = GreedyDecoder(labels=labels)\n",
    "est_transcript = decoder(emission)\n",
    "print(est_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Evaluation\n",
    "\n",
    "To evaluate the quality of the transcript above, we need to compare it to some reference. More precisely, we can compute the [word error rate](https://en.wikipedia.org/wiki/Word_error_rate) (WER) between the reference and estimated transcripts. The WER is a measure of speech recognition performance expressed in % (lower is better) based on the Levenshtein distance, which allows to account for the fact that the true and estimated transcripts might have different lengths; more info in the course.\n",
    "\n",
    "For the example we used until now, the true transcript is stored in the file `asr-dataset/transcription/61-70968-0001.txt` (note the file name is similar as the audio, except for the subfolder and extension).\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 1**</span>. Load the content of this file to get the true transcript, and preprocess it to the same format as the estimated transcript above (i.e., use the `.lower()` and `.split()` methods). Display the true transcript. Then, using the provided `get_wer` function, compute and display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wer(true_transcript, est_transcript):\n",
    "    wer = torchaudio.functional.edit_distance(true_transcript, est_transcript) / len(true_transcript)\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a folder\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 2**</span>. Process the entire dataset folder (=the 100 files) using the wav2vec2 acoustic model and the greedy decoder above, and compute and display the mean WER over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a function below that returns the list of all files in a directory\n",
    "def find_files(directory, pattern='*.wav'):\n",
    "    \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            files.append(filename)\n",
    "    files = sorted(files)\n",
    "    return files\n",
    "\n",
    "# Get the list of audio files in the asr dataset\n",
    "audio_files = find_files(data_speech_dir)\n",
    "print(audio_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the noise\n",
    "\n",
    "ASR systems are expected to be sensitive to the noise: while a model might work well on a clean recording, its performance will drop in real-life conditions where speech is contaminated with noise. We can simulate such a scenario by adding white noise to our speech waveforms.\n",
    "\n",
    "The amount of noise is controlled by the *signal-to-noise ratio* (SNR), expressed in dB. The lower the SNR, the noisier the mixture: a clean speech signal corresponds to SNR=+$\\infty$, while a pure noise signal corresponds to SNR=-$\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions below allow to generate white noise, and add it to the clean speech at a certain SNR level\n",
    "\n",
    "def adjust_noise_snr(\n",
    "    speech,\n",
    "    noise,\n",
    "    snr_dB,\n",
    "    silence_threshold=0.01,\n",
    "):\n",
    "\n",
    "    # Filter out silence from speech signal\n",
    "    speech_wo_silence = speech[torch.abs(speech) > silence_threshold]\n",
    "\n",
    "    # Calculate the total energy of the speech and noise signals\n",
    "    speech_power = torch.sum(torch.square(speech_wo_silence))\n",
    "    noise_power = torch.sum(torch.square(noise))\n",
    "\n",
    "    # Compute gain factor to achieve desired SNR\n",
    "    gain_factor = torch.sqrt((speech_power / noise_power) * 10 ** (-snr_dB / 10))\n",
    "\n",
    "    # Scale the noise accordingly\n",
    "    scaled_noise = gain_factor * noise\n",
    "\n",
    "    return scaled_noise\n",
    "\n",
    "\n",
    "def add_noise(speech, snr_dB=0):\n",
    "    # Generate random white noise\n",
    "    noise = torch.randn_like(speech)\n",
    "    # Adjust noise level\n",
    "    noise = adjust_noise_snr(speech, noise, snr_dB)\n",
    "    # Add the noise to the clean speech\n",
    "    speech_noisy = speech + noise\n",
    "    return speech_noisy, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the original audio signal at SNR = 0 dB\n",
    "speech_noisy_0dB, noise_OdB = add_noise(waveform, snr_dB=0)\n",
    "IPython.display.Audio(data=speech_noisy_0dB, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the original audio signal at SNR = 5 dB\n",
    "speech_noisy_5dB, noise_5dB = add_noise(waveform, snr_dB=5)\n",
    "IPython.display.Audio(data=speech_noisy_5dB, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the noisy signals (vs. the clean one)\n",
    "plt.figure()\n",
    "xt = torch.arange(waveform.size(1)) / sr\n",
    "plt.plot(xt, speech_noisy_0dB.T, label='noisy, SNR=0 dB')\n",
    "plt.plot(xt, speech_noisy_5dB.T, label='noisy, SNR=5 dB')\n",
    "plt.plot(xt, waveform.T, label='clean')\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 3**</span>. For each SNR $\\in [-5, 0, 5, 10]$ dB, add noise to the files in the dataset, perform ASR and compute/display the mean WER. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the amount of training data\n",
    "\n",
    "So far, we have used a model that was fine-tuned for ASR on 100 hours of speech, but alternative models exist in torchaudio (recall the list of pipelines is [here](https://pytorch.org/audio/stable/pipelines.html#id36)).\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 4**</span>. Perform ASR on the dataset and compute the WER for models that are fine-tuned with either less or more speech data, and comment. Be careful of using models using the same architecture (basic wav2vec2) for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that many other pretrained models are readily available in torchaudio, such as a:\n",
    "- larger models (e.g., `'WAV2VEC2_ASR_LARGE_10M'`)\n",
    "- model trained using another dataset (e.g., `'WAV2VEC2_ASR_LARGE_LV60K_10M'`)\n",
    "- alternative architectures (e.g., `'HUBERT_ASR_LARGE'`).\n",
    "\n",
    "Feel free to experiment with these!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
